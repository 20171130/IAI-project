{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "from utilities.data_structures.Config import Config\n",
    "from agents.DQN_agents.DDQN import DDQN\n",
    "from agents.DQN_agents.DQN_With_Fixed_Q_Targets import DQN_With_Fixed_Q_Targets\n",
    "\n",
    "config = Config()\n",
    "config.seed = 1\n",
    "config.num_episodes_to_run = 1000\n",
    "config.file_to_save_data_results = \"results/data_and_graphs/Cart_Pole_Results_Data.pkl\"\n",
    "config.file_to_save_results_graph = \"results/data_and_graphs/Cart_Pole_Results_Graph.png\"\n",
    "config.show_solution_score = False\n",
    "config.visualise_individual_results = False\n",
    "config.visualise_overall_agent_results = True\n",
    "config.standard_deviation_results = 1.0\n",
    "config.runs_per_agent = 1\n",
    "config.use_GPU = True\n",
    "config.overwrite_existing_results_file = False\n",
    "config.randomise_random_seed = True\n",
    "config.save_model = True\n",
    "\n",
    "\n",
    "hyperparameters = {\n",
    "    \"DQN_Agents\": {\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"batch_size\": 32,\n",
    "        \"buffer_size\": 400000,\n",
    "        \"epsilon\": 0.01,\n",
    "        \"epsilon_decay_rate_denominator\": 1,\n",
    "        \"discount_rate\": 0.99,\n",
    "        \"tau\": 1e-4,\n",
    "        \"alpha_prioritised_replay\": 0.6,\n",
    "        \"beta_prioritised_replay\": 0.1,\n",
    "        \"incremental_td_error\": 1e-8,\n",
    "        \"update_every_n_steps\": 1,\n",
    "        \"learning_iterations\": 1,\n",
    "        \"final_layer_activation\": \"None\",\n",
    "        \"batch_norm\": False,\n",
    "        \"gradient_clipping_norm\": 0.7,\n",
    "        \"clip_rewards\": False\n",
    "    },\n",
    "    \"Stochastic_Policy_Search_Agents\": {\n",
    "        \"policy_network_type\": \"Linear\",\n",
    "        \"noise_scale_start\": 1e-2,\n",
    "        \"noise_scale_min\": 1e-3,\n",
    "        \"noise_scale_max\": 2.0,\n",
    "        \"noise_scale_growth_factor\": 2.0,\n",
    "        \"stochastic_action_decision\": False,\n",
    "        \"num_policies\": 10,\n",
    "        \"episodes_per_policy\": 1,\n",
    "        \"num_policies_to_keep\": 5,\n",
    "        \"clip_rewards\": False\n",
    "    },\n",
    "    \"Policy_Gradient_Agents\": {\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"linear_hidden_units\": [20, 20],\n",
    "        \"final_layer_activation\": \"SOFTMAX\",\n",
    "        \"learning_iterations_per_round\": 5,\n",
    "        \"discount_rate\": 0.99,\n",
    "        \"batch_norm\": False,\n",
    "        \"clip_epsilon\": 0.1,\n",
    "        \"episodes_per_learning_round\": 4,\n",
    "        \"normalise_rewards\": True,\n",
    "        \"gradient_clipping_norm\": 7.0,\n",
    "        \"mu\": 0.0, #only required for continuous action games\n",
    "        \"theta\": 0.0, #only required for continuous action games\n",
    "        \"sigma\": 0.0, #only required for continuous action games\n",
    "        \"epsilon_decay_rate_denominator\": 1.0,\n",
    "        \"clip_rewards\": False\n",
    "    },\n",
    "\n",
    "    \"Actor_Critic_Agents\":  {\n",
    "\n",
    "        \"learning_rate\": 0.005,\n",
    "        \"linear_hidden_units\": [20, 10],\n",
    "        \"final_layer_activation\": [\"SOFTMAX\", None],\n",
    "        \"gradient_clipping_norm\": 5.0,\n",
    "        \"discount_rate\": 0.99,\n",
    "        \"epsilon_decay_rate_denominator\": 1.0,\n",
    "        \"normalise_rewards\": True,\n",
    "        \"exploration_worker_difference\": 2.0,\n",
    "        \"clip_rewards\": False,\n",
    "\n",
    "        \"Actor\": {\n",
    "            \"learning_rate\": 0.0003,\n",
    "            \"linear_hidden_units\": [64, 64],\n",
    "            \"final_layer_activation\": \"Softmax\",\n",
    "            \"batch_norm\": False,\n",
    "            \"tau\": 0.005,\n",
    "            \"gradient_clipping_norm\": 5,\n",
    "            \"initialiser\": \"Xavier\"\n",
    "        },\n",
    "\n",
    "        \"Critic\": {\n",
    "            \"learning_rate\": 0.0003,\n",
    "            \"linear_hidden_units\": [64, 64],\n",
    "            \"final_layer_activation\": None,\n",
    "            \"batch_norm\": False,\n",
    "            \"buffer_size\": 1000000,\n",
    "            \"tau\": 0.005,\n",
    "            \"gradient_clipping_norm\": 5,\n",
    "            \"initialiser\": \"Xavier\"\n",
    "        },\n",
    "\n",
    "        \"min_steps_before_learning\": 400,\n",
    "        \"batch_size\": 256,\n",
    "        \"discount_rate\": 0.99,\n",
    "        \"mu\": 0.0, #for O-H noise\n",
    "        \"theta\": 0.15, #for O-H noise\n",
    "        \"sigma\": 0.25, #for O-H noise\n",
    "        \"action_noise_std\": 0.2,  # for TD3\n",
    "        \"action_noise_clipping_range\": 0.5,  # for TD3\n",
    "        \"update_every_n_steps\": 1,\n",
    "        \"learning_updates_per_learning_session\": 1,\n",
    "        \"automatically_tune_entropy_hyperparameter\": True,\n",
    "        \"entropy_term_weight\": None,\n",
    "        \"add_extra_noise\": False,\n",
    "        \"do_evaluation_iterations\": True\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Env wrapper, currently Breakout and CartPole implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \"\"\"input resized to [84, 84, 1]\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size = 8, stride = 4)\n",
    "        # 20 20\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size = 4, stride = 2)\n",
    "        # 9 9\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size = 3, stride = 1)\n",
    "        # 7 7\n",
    "        self.fc1 = nn.Linear(7*7*64, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = x.view(-1, 7*7*64)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(4, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class Wrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, environment):\n",
    "        super().__init__(environment)\n",
    "\n",
    "    def observation(self, state):\n",
    "        return self.process(state)\n",
    "    \n",
    "    def step(self, action):\n",
    "        state, reward, done, info = super().step(action)\n",
    "        if info[\"ale.lives\"]!=5:\n",
    "            done = True\n",
    "        return state, reward, done, info\n",
    "    \n",
    "    def process(self, state):\n",
    "        state = Image.fromarray(state)\n",
    "        state = state.crop(box = [0, 34, 160, 194])\n",
    "        state = state.convert(mode=\"L\")\n",
    "        state = state.resize((84, 84))\n",
    "        state = np.array(state)\n",
    "        return state.reshape(1, 84, 84)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wuruihai/anaconda3/envs/py368torch12/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/wuruihai/anaconda3/envs/py368torch12/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/wuruihai/anaconda3/envs/py368torch12/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/wuruihai/anaconda3/envs/py368torch12/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/wuruihai/anaconda3/envs/py368torch12/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/wuruihai/anaconda3/envs/py368torch12/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TITLE  BreakoutDeterministic\n",
      "Score required to win set to infinity therefore no learning rate annealing will happen\n"
     ]
    }
   ],
   "source": [
    "config.model_class = CNN\n",
    "config.environment = Wrapper(gym.make('BreakoutDeterministic-v4'))\n",
    "#config.environment = gym.make('CartPole-v0')\n",
    "config.hyperparameters = hyperparameters[\"DQN_Agents\"]\n",
    "agent = DDQN(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Episode 110, Score:  4.00, Max score seen:  7.00, Rolling score:  3.46, Max rolling score seen:  3.47\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\""
     ]
    }
   ],
   "source": [
    "# donnot reset the epsiode number (that resets eps-greddy) increase the n_episodes instead\n",
    "game_scores, rolling_scores, time_taken = agent.run_n_episodes(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "agent.q_network_optimizer = optim.Adam(agent.q_network_local.parameters(), lr =1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S/L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"./checkpoint6\")\n",
    "agent.q_network_local.load_state_dict(checkpoint['model_dict'])\n",
    "agent.q_network_target.load_state_dict(checkpoint['model_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'model_dict': agent.q_network_local.state_dict()}, \"checkpoint6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "refer to https://openai.com/blog/openai-baselines-dqn/, for best practices.\n",
    "\n",
    "The MNIST and CIFAR of RL\n",
    "\n",
    "4 dim parameterized CartPole can be solved (keeping balance for more than 200 frames on average) within 500 episodes\n",
    "\n",
    "As for image input, BreakoutDeterministic-v4. 10M transitions is a common setting, and it is okay to have no improvements during the first 1M transitions (perhaps due to high eps)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes\n",
    "Implemented real eps-greedy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
