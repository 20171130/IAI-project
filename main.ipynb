{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "from utilities.data_structures.Config import Config\n",
    "from agents.DQN_agents.DDQN import DDQN\n",
    "from agents.DQN_agents.DQN_With_Fixed_Q_Targets import DQN_With_Fixed_Q_Targets\n",
    "\n",
    "config = Config()\n",
    "config.seed = 1\n",
    "config.num_episodes_to_run = 450\n",
    "config.file_to_save_data_results = \"results/data_and_graphs/Cart_Pole_Results_Data.pkl\"\n",
    "config.file_to_save_results_graph = \"results/data_and_graphs/Cart_Pole_Results_Graph.png\"\n",
    "config.show_solution_score = False\n",
    "config.visualise_individual_results = False\n",
    "config.visualise_overall_agent_results = True\n",
    "config.standard_deviation_results = 1.0\n",
    "config.runs_per_agent = 1\n",
    "config.use_GPU = True\n",
    "config.overwrite_existing_results_file = False\n",
    "config.randomise_random_seed = True\n",
    "config.save_model = False\n",
    "\n",
    "\n",
    "hyperparameters = {\n",
    "    \"DQN_Agents\": {\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"batch_size\": 256,\n",
    "        \"buffer_size\": 40000,\n",
    "        \"epsilon\": 1.0,\n",
    "        \"epsilon_decay_rate_denominator\": 1,\n",
    "        \"discount_rate\": 0.99,\n",
    "        \"tau\": 0.01,\n",
    "        \"alpha_prioritised_replay\": 0.6,\n",
    "        \"beta_prioritised_replay\": 0.1,\n",
    "        \"incremental_td_error\": 1e-8,\n",
    "        \"update_every_n_steps\": 1,\n",
    "        \"learning_iterations\": 1,\n",
    "        \"linear_hidden_units\": [30, 15],\n",
    "        \"final_layer_activation\": \"None\",\n",
    "        \"batch_norm\": False,\n",
    "        \"gradient_clipping_norm\": 0.7,\n",
    "        \"clip_rewards\": False\n",
    "    },\n",
    "    \"Stochastic_Policy_Search_Agents\": {\n",
    "        \"policy_network_type\": \"Linear\",\n",
    "        \"noise_scale_start\": 1e-2,\n",
    "        \"noise_scale_min\": 1e-3,\n",
    "        \"noise_scale_max\": 2.0,\n",
    "        \"noise_scale_growth_factor\": 2.0,\n",
    "        \"stochastic_action_decision\": False,\n",
    "        \"num_policies\": 10,\n",
    "        \"episodes_per_policy\": 1,\n",
    "        \"num_policies_to_keep\": 5,\n",
    "        \"clip_rewards\": False\n",
    "    },\n",
    "    \"Policy_Gradient_Agents\": {\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"linear_hidden_units\": [20, 20],\n",
    "        \"final_layer_activation\": \"SOFTMAX\",\n",
    "        \"learning_iterations_per_round\": 5,\n",
    "        \"discount_rate\": 0.99,\n",
    "        \"batch_norm\": False,\n",
    "        \"clip_epsilon\": 0.1,\n",
    "        \"episodes_per_learning_round\": 4,\n",
    "        \"normalise_rewards\": True,\n",
    "        \"gradient_clipping_norm\": 7.0,\n",
    "        \"mu\": 0.0, #only required for continuous action games\n",
    "        \"theta\": 0.0, #only required for continuous action games\n",
    "        \"sigma\": 0.0, #only required for continuous action games\n",
    "        \"epsilon_decay_rate_denominator\": 1.0,\n",
    "        \"clip_rewards\": False\n",
    "    },\n",
    "\n",
    "    \"Actor_Critic_Agents\":  {\n",
    "\n",
    "        \"learning_rate\": 0.005,\n",
    "        \"linear_hidden_units\": [20, 10],\n",
    "        \"final_layer_activation\": [\"SOFTMAX\", None],\n",
    "        \"gradient_clipping_norm\": 5.0,\n",
    "        \"discount_rate\": 0.99,\n",
    "        \"epsilon_decay_rate_denominator\": 1.0,\n",
    "        \"normalise_rewards\": True,\n",
    "        \"exploration_worker_difference\": 2.0,\n",
    "        \"clip_rewards\": False,\n",
    "\n",
    "        \"Actor\": {\n",
    "            \"learning_rate\": 0.0003,\n",
    "            \"linear_hidden_units\": [64, 64],\n",
    "            \"final_layer_activation\": \"Softmax\",\n",
    "            \"batch_norm\": False,\n",
    "            \"tau\": 0.005,\n",
    "            \"gradient_clipping_norm\": 5,\n",
    "            \"initialiser\": \"Xavier\"\n",
    "        },\n",
    "\n",
    "        \"Critic\": {\n",
    "            \"learning_rate\": 0.0003,\n",
    "            \"linear_hidden_units\": [64, 64],\n",
    "            \"final_layer_activation\": None,\n",
    "            \"batch_norm\": False,\n",
    "            \"buffer_size\": 1000000,\n",
    "            \"tau\": 0.005,\n",
    "            \"gradient_clipping_norm\": 5,\n",
    "            \"initialiser\": \"Xavier\"\n",
    "        },\n",
    "\n",
    "        \"min_steps_before_learning\": 400,\n",
    "        \"batch_size\": 256,\n",
    "        \"discount_rate\": 0.99,\n",
    "        \"mu\": 0.0, #for O-H noise\n",
    "        \"theta\": 0.15, #for O-H noise\n",
    "        \"sigma\": 0.25, #for O-H noise\n",
    "        \"action_noise_std\": 0.2,  # for TD3\n",
    "        \"action_noise_clipping_range\": 0.5,  # for TD3\n",
    "        \"update_every_n_steps\": 1,\n",
    "        \"learning_updates_per_learning_session\": 1,\n",
    "        \"automatically_tune_entropy_hyperparameter\": True,\n",
    "        \"entropy_term_weight\": None,\n",
    "        \"add_extra_noise\": False,\n",
    "        \"do_evaluation_iterations\": True\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \"\"\"input resized to [84, 84, 1]\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size = 8, stride = 4)\n",
    "        # 20 20\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size = 4, stride = 2)\n",
    "        # 9 9\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size = 3, stride = 1)\n",
    "        # 7 7\n",
    "        self.fc1 = nn.Linear(7*7*64, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = x.view(-1, 7*7*64)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class Wrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, environment):\n",
    "        super().__init__(environment)\n",
    "\n",
    "    def observation(self, state):\n",
    "        return self.process(state)\n",
    "        \n",
    "    def process(self, state):\n",
    "        state = Image.fromarray(state)\n",
    "        state = state.crop(box = [0, 34, 160, 194])\n",
    "        state = state.convert(mode=\"L\")\n",
    "        state = state.resize((84, 84))\n",
    "        state = np.array(state)\n",
    "        return state.reshape(1, 84, 84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TITLE  Breakout\n",
      "Score required to win set to infinity therefore no learning rate annealing will happen\n",
      " Episode 2, Score:  0.00, Max score seen:  0.00, Rolling score:  0.00, Max rolling score seen:  0.00\""
     ]
    }
   ],
   "source": [
    "config.model_class = Model\n",
    "config.environment = Wrapper(gym.make('Breakout-v0'))\n",
    "config.hyperparameters = hyperparameters[\"DQN_Agents\"]\n",
    "agent = DDQN(config)\n",
    "game_scores, rolling_scores, time_taken = agent.run_n_episodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b,c,d,e =agent.sample_experiences()\n",
    "agent.state.shape\n",
    "x = np.array(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified: \n",
    "        disable trails in base_agent\n",
    "        disable reward threshold for breakout in base_agent\n",
    "        support custom network in Config, DQN, DQN_fixed_Q \n",
    "        np.vstack behaves differently for 1D and nD tensors, so changed in replay_buffer, now experiences has a batch dim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
