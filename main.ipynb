{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage\n",
    "\n",
    "change config.device to set the GPU to use\n",
    "change the wrapper class the set up the discrete action space\n",
    "change the initialization segement to set config.hyperparameters about state and action size\n",
    "\n",
    "requires tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "from utilities.data_structures.Config import Config\n",
    "from agents.DQN_agents.DDQN import DDQN\n",
    "from agents.DQN_agents.DQN_With_Fixed_Q_Targets import DQN_With_Fixed_Q_Targets\n",
    "\n",
    "config = Config()\n",
    "config.seed = 1\n",
    "config.num_episodes_to_run = 1000\n",
    "config.file_to_save_data_results = \"results/data_and_graphs/Cart_Pole_Results_Data.pkl\"\n",
    "config.file_to_save_results_graph = \"results/data_and_graphs/Cart_Pole_Results_Graph.png\"\n",
    "config.show_solution_score = False\n",
    "config.visualise_individual_results = False\n",
    "config.visualise_overall_agent_results = True\n",
    "config.standard_deviation_results = 1.0\n",
    "config.runs_per_agent = 1\n",
    "config.use_GPU = True\n",
    "config.overwrite_existing_results_file = False\n",
    "config.randomise_random_seed = True\n",
    "config.save_model = True\n",
    "config.device = 2\n",
    "\n",
    "\n",
    "hyperparameters = {\n",
    "    \"DQN_Agents\": {\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"batch_size\": 32,\n",
    "        \"buffer_size\": 200000,\n",
    "        \"epsilon\": 0.01,\n",
    "        \"epsilon_decay_rate_denominator\": 1,\n",
    "        \"discount_rate\": 0.99,\n",
    "        \"tau\": 1e-4,\n",
    "        \"alpha_prioritised_replay\": 0.6,\n",
    "        \"beta_prioritised_replay\": 0.1,\n",
    "        \"incremental_td_error\": 1e-8,\n",
    "        \"update_every_n_steps\": 1,\n",
    "        \"learning_iterations\": 1,\n",
    "        \"final_layer_activation\": \"None\",\n",
    "        \"batch_norm\": False,\n",
    "        \"gradient_clipping_norm\": 0.7,\n",
    "        \"clip_rewards\": False\n",
    "    },\n",
    "    \"Stochastic_Policy_Search_Agents\": {\n",
    "        \"policy_network_type\": \"Linear\",\n",
    "        \"noise_scale_start\": 1e-2,\n",
    "        \"noise_scale_min\": 1e-3,\n",
    "        \"noise_scale_max\": 2.0,\n",
    "        \"noise_scale_growth_factor\": 2.0,\n",
    "        \"stochastic_action_decision\": False,\n",
    "        \"num_policies\": 10,\n",
    "        \"episodes_per_policy\": 1,\n",
    "        \"num_policies_to_keep\": 5,\n",
    "        \"clip_rewards\": False\n",
    "    },\n",
    "    \"Policy_Gradient_Agents\": {\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"linear_hidden_units\": [20, 20],\n",
    "        \"final_layer_activation\": \"SOFTMAX\",\n",
    "        \"learning_iterations_per_round\": 5,\n",
    "        \"discount_rate\": 0.99,\n",
    "        \"batch_norm\": False,\n",
    "        \"clip_epsilon\": 0.1,\n",
    "        \"episodes_per_learning_round\": 4,\n",
    "        \"normalise_rewards\": True,\n",
    "        \"gradient_clipping_norm\": 7.0,\n",
    "        \"mu\": 0.0, #only required for continuous action games\n",
    "        \"theta\": 0.0, #only required for continuous action games\n",
    "        \"sigma\": 0.0, #only required for continuous action games\n",
    "        \"epsilon_decay_rate_denominator\": 1.0,\n",
    "        \"clip_rewards\": False\n",
    "    },\n",
    "\n",
    "    \"Actor_Critic_Agents\":  {\n",
    "\n",
    "        \"learning_rate\": 0.005,\n",
    "        \"linear_hidden_units\": [20, 10],\n",
    "        \"final_layer_activation\": [\"SOFTMAX\", None],\n",
    "        \"gradient_clipping_norm\": 5.0,\n",
    "        \"discount_rate\": 0.99,\n",
    "        \"epsilon_decay_rate_denominator\": 1.0,\n",
    "        \"normalise_rewards\": True,\n",
    "        \"exploration_worker_difference\": 2.0,\n",
    "        \"clip_rewards\": False,\n",
    "\n",
    "        \"Actor\": {\n",
    "            \"learning_rate\": 0.0003,\n",
    "            \"linear_hidden_units\": [64, 64],\n",
    "            \"final_layer_activation\": \"Softmax\",\n",
    "            \"batch_norm\": False,\n",
    "            \"tau\": 0.005,\n",
    "            \"gradient_clipping_norm\": 5,\n",
    "            \"initialiser\": \"Xavier\"\n",
    "        },\n",
    "\n",
    "        \"Critic\": {\n",
    "            \"learning_rate\": 0.0003,\n",
    "            \"linear_hidden_units\": [64, 64],\n",
    "            \"final_layer_activation\": None,\n",
    "            \"batch_norm\": False,\n",
    "            \"buffer_size\": 1000000,\n",
    "            \"tau\": 0.005,\n",
    "            \"gradient_clipping_norm\": 5,\n",
    "            \"initialiser\": \"Xavier\"\n",
    "        },\n",
    "\n",
    "        \"min_steps_before_learning\": 400,\n",
    "        \"batch_size\": 256,\n",
    "        \"discount_rate\": 0.99,\n",
    "        \"mu\": 0.0, #for O-H noise\n",
    "        \"theta\": 0.15, #for O-H noise\n",
    "        \"sigma\": 0.25, #for O-H noise\n",
    "        \"action_noise_std\": 0.2,  # for TD3\n",
    "        \"action_noise_clipping_range\": 0.5,  # for TD3\n",
    "        \"update_every_n_steps\": 1,\n",
    "        \"learning_updates_per_learning_session\": 1,\n",
    "        \"automatically_tune_entropy_hyperparameter\": True,\n",
    "        \"entropy_term_weight\": None,\n",
    "        \"add_extra_noise\": False,\n",
    "        \"do_evaluation_iterations\": True\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Env wrapper, currently Breakout and CartPole implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(config.hyperparameters[\"state_dim\"], 32)\n",
    "        self.fc2 = nn.Linear(32, config.hyperparameters[\"action_dim\"])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class Wrapper(gym.ActionWrapper):\n",
    "    def __init__(self, environment):\n",
    "        super().__init__(environment)\n",
    "        self.actions = [[-1], [-0.3] , [0], [0.3], [1]]\n",
    "        \n",
    "    def action(self, a):\n",
    "        return self.actions[a]\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\" 粗粒度规划鼓励探索,否则mountain car不可解 \"\"\"\n",
    "        total = 0\n",
    "        for i in range(10):\n",
    "            state, reward, done, info = super().step(action)\n",
    "            total += reward\n",
    "            if done:\n",
    "                break\n",
    "        return state, total, done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TITLE  MountainCarContinuous\n"
     ]
    }
   ],
   "source": [
    "record = False\n",
    "load = False\n",
    "if record:\n",
    "    from pyvirtualdisplay import Display\n",
    "    # opens a virtual monitor, for outputting videos\n",
    "    monitor = Display(visible=0, size=(210, 160))\n",
    "    monitor.start()\n",
    "\n",
    "\n",
    "config.model_class = NN\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "env = Wrapper(env)\n",
    "config.hyperparameters[\"state_dim\"] = env.reset().size\n",
    "config.hyperparameters[\"action_dim\"] = 5\n",
    "\n",
    "\"\"\"\n",
    "config.model_class = NN\n",
    "env = gym.make('CartPole-v0')\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "if record:\n",
    "    env =  gym.wrappers.Monitor(env, \"tmp\", force=True, video_callable=lambda episode_id: True)\n",
    "config.environment = env\n",
    "#config.environment = gym.make('CartPole-v0')\n",
    "config.hyperparameters = hyperparameters[\"DQN_Agents\"]\n",
    "agent = DDQN(config)\n",
    "\n",
    "if load:\n",
    "    checkpoint = torch.load(\"./checkpoint1\")\n",
    "    agent.q_network_local.load_state_dict(checkpoint['model_dict'])\n",
    "    agent.q_network_target.load_state_dict(checkpoint['model_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\u001b[91m\u001b[1mDDQN did not achieve required score \n",
      "\u001b[0m\u001b[0m\n",
      " Episode 11, Score:  92.60, Max score seen:  92.60, Rolling score: -8.18, Max rolling score seen:  71.799\"\"\"\"\"\"\"\"\""
     ]
    }
   ],
   "source": [
    "save = True\n",
    "agent.set_random_seeds(190)\n",
    "# donnot reset the epsiode number (that resets eps-greddy) increase the n_episodes instead\n",
    "for i in range(50):\n",
    "    game_scores, rolling_scores, time_taken = agent.run_n_episodes(i*1000)\n",
    "    if save:\n",
    "        torch.save({'model_dict': agent.q_network_local.state_dict()}, \"checkpoint%d\"%i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S/L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"./checkpoint1\")\n",
    "agent.q_network_local.load_state_dict(checkpoint['model_dict'])\n",
    "agent.q_network_target.load_state_dict(checkpoint['model_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'model_dict': agent.q_network_local.state_dict()}, \"checkpoint4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "refer to https://openai.com/blog/openai-baselines-dqn/, for best practices and the 2013 paper https://arxiv.org/pdf/1312.5602.pdf).\n",
    "\n",
    "The MNIST and CIFAR of RL\n",
    "\n",
    "4 dim parameterized CartPole can be solved (keeping balance for more than 200 frames on average) within 500 episodes.\n",
    "LR 1e-2, tau 1e-2.\n",
    "\n",
    "Pong-ram-v0 and Pong-v0 image\n",
    "\n",
    "As for image input, BreakoutDeterministic-v4. 10M transitions is a common setting, and it is okay to have no improvements during the first 1M transitions (perhaps due to high eps). LR 1e-5, tau 1e-4, average of 11 is obtained using a single frame(incomplete state). DeepMind takes last 4 frames as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes\n",
    "Implemented real eps-greedy\n",
    "now supports setting the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.49709999,  0.        ])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-0.49671983, -0.00195616]), -0.1, False, {})\n"
     ]
    }
   ],
   "source": [
    "for i in range(30):\n",
    "    tmp = env.step(4)\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
