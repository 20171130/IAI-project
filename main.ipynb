{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "from utilities.data_structures.Config import Config\n",
    "from agents.DQN_agents.DDQN import DDQN\n",
    "from agents.DQN_agents.DQN_With_Fixed_Q_Targets import DQN_With_Fixed_Q_Targets\n",
    "\n",
    "config = Config()\n",
    "config.seed = 1\n",
    "config.num_episodes_to_run = 1000\n",
    "config.file_to_save_data_results = \"results/data_and_graphs/Cart_Pole_Results_Data.pkl\"\n",
    "config.file_to_save_results_graph = \"results/data_and_graphs/Cart_Pole_Results_Graph.png\"\n",
    "config.show_solution_score = False\n",
    "config.visualise_individual_results = False\n",
    "config.visualise_overall_agent_results = True\n",
    "config.standard_deviation_results = 1.0\n",
    "config.runs_per_agent = 1\n",
    "config.use_GPU = True\n",
    "config.overwrite_existing_results_file = False\n",
    "config.randomise_random_seed = True\n",
    "config.save_model = True\n",
    "config.device = 2\n",
    "\n",
    "\n",
    "hyperparameters = {\n",
    "    \"DQN_Agents\": {\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"batch_size\": 32,\n",
    "        \"buffer_size\": 200000,\n",
    "        \"epsilon\": 0.01,\n",
    "        \"epsilon_decay_rate_denominator\": 1,\n",
    "        \"discount_rate\": 0.99,\n",
    "        \"tau\": 1e-4,\n",
    "        \"alpha_prioritised_replay\": 0.6,\n",
    "        \"beta_prioritised_replay\": 0.1,\n",
    "        \"incremental_td_error\": 1e-8,\n",
    "        \"update_every_n_steps\": 1,\n",
    "        \"learning_iterations\": 1,\n",
    "        \"final_layer_activation\": \"None\",\n",
    "        \"batch_norm\": False,\n",
    "        \"gradient_clipping_norm\": 0.7,\n",
    "        \"clip_rewards\": False\n",
    "    },\n",
    "    \"Stochastic_Policy_Search_Agents\": {\n",
    "        \"policy_network_type\": \"Linear\",\n",
    "        \"noise_scale_start\": 1e-2,\n",
    "        \"noise_scale_min\": 1e-3,\n",
    "        \"noise_scale_max\": 2.0,\n",
    "        \"noise_scale_growth_factor\": 2.0,\n",
    "        \"stochastic_action_decision\": False,\n",
    "        \"num_policies\": 10,\n",
    "        \"episodes_per_policy\": 1,\n",
    "        \"num_policies_to_keep\": 5,\n",
    "        \"clip_rewards\": False\n",
    "    },\n",
    "    \"Policy_Gradient_Agents\": {\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"linear_hidden_units\": [20, 20],\n",
    "        \"final_layer_activation\": \"SOFTMAX\",\n",
    "        \"learning_iterations_per_round\": 5,\n",
    "        \"discount_rate\": 0.99,\n",
    "        \"batch_norm\": False,\n",
    "        \"clip_epsilon\": 0.1,\n",
    "        \"episodes_per_learning_round\": 4,\n",
    "        \"normalise_rewards\": True,\n",
    "        \"gradient_clipping_norm\": 7.0,\n",
    "        \"mu\": 0.0, #only required for continuous action games\n",
    "        \"theta\": 0.0, #only required for continuous action games\n",
    "        \"sigma\": 0.0, #only required for continuous action games\n",
    "        \"epsilon_decay_rate_denominator\": 1.0,\n",
    "        \"clip_rewards\": False\n",
    "    },\n",
    "\n",
    "    \"Actor_Critic_Agents\":  {\n",
    "\n",
    "        \"learning_rate\": 0.005,\n",
    "        \"linear_hidden_units\": [20, 10],\n",
    "        \"final_layer_activation\": [\"SOFTMAX\", None],\n",
    "        \"gradient_clipping_norm\": 5.0,\n",
    "        \"discount_rate\": 0.99,\n",
    "        \"epsilon_decay_rate_denominator\": 1.0,\n",
    "        \"normalise_rewards\": True,\n",
    "        \"exploration_worker_difference\": 2.0,\n",
    "        \"clip_rewards\": False,\n",
    "\n",
    "        \"Actor\": {\n",
    "            \"learning_rate\": 0.0003,\n",
    "            \"linear_hidden_units\": [64, 64],\n",
    "            \"final_layer_activation\": \"Softmax\",\n",
    "            \"batch_norm\": False,\n",
    "            \"tau\": 0.005,\n",
    "            \"gradient_clipping_norm\": 5,\n",
    "            \"initialiser\": \"Xavier\"\n",
    "        },\n",
    "\n",
    "        \"Critic\": {\n",
    "            \"learning_rate\": 0.0003,\n",
    "            \"linear_hidden_units\": [64, 64],\n",
    "            \"final_layer_activation\": None,\n",
    "            \"batch_norm\": False,\n",
    "            \"buffer_size\": 1000000,\n",
    "            \"tau\": 0.005,\n",
    "            \"gradient_clipping_norm\": 5,\n",
    "            \"initialiser\": \"Xavier\"\n",
    "        },\n",
    "\n",
    "        \"min_steps_before_learning\": 400,\n",
    "        \"batch_size\": 256,\n",
    "        \"discount_rate\": 0.99,\n",
    "        \"mu\": 0.0, #for O-H noise\n",
    "        \"theta\": 0.15, #for O-H noise\n",
    "        \"sigma\": 0.25, #for O-H noise\n",
    "        \"action_noise_std\": 0.2,  # for TD3\n",
    "        \"action_noise_clipping_range\": 0.5,  # for TD3\n",
    "        \"update_every_n_steps\": 1,\n",
    "        \"learning_updates_per_learning_session\": 1,\n",
    "        \"automatically_tune_entropy_hyperparameter\": True,\n",
    "        \"entropy_term_weight\": None,\n",
    "        \"add_extra_noise\": False,\n",
    "        \"do_evaluation_iterations\": True\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Env wrapper, currently Breakout and CartPole implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = 3\n",
    "class CNN(nn.Module):\n",
    "    \"\"\"input resized to [84, 84, 1]\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(frames, 32, kernel_size = 8, stride = 4)\n",
    "        # 20 20\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size = 4, stride = 2)\n",
    "        # 9 9\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size = 3, stride = 1)\n",
    "        # 7 7\n",
    "        self.fc1 = nn.Linear(7*7*64, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = x.view(-1, 7*7*64)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(4, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class Wrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, environment):\n",
    "        super().__init__(environment)\n",
    "        \n",
    "    def reset(self):\n",
    "        state = super().reset()\n",
    "        self.state = np.concatenate([state for i in range(frames)], axis=0)\n",
    "        return self.state\n",
    "\n",
    "    def observation(self, state):\n",
    "        return self.process(state)\n",
    "    \n",
    "    def step(self, action):\n",
    "        state, reward, done, info = super().step(action)\n",
    "        self.state = np.concatenate([self.state[1:frames], state], axis=0)\n",
    "        if info[\"ale.lives\"]!=5:\n",
    "            done = True\n",
    "        return self.state, reward, done, info\n",
    "    \n",
    "    def process(self, state):\n",
    "        state = Image.fromarray(state)\n",
    "        state = state.crop(box = [0, 34, 160, 194])\n",
    "        state = state.convert(mode=\"L\")\n",
    "        state = state.resize((84, 84))\n",
    "        state = np.array(state)\n",
    "        return state.reshape(1, 84, 84)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TITLE  BreakoutDeterministic\n",
      "Score required to win set to infinity therefore no learning rate annealing will happen\n"
     ]
    }
   ],
   "source": [
    "record = False\n",
    "load = True\n",
    "if record:\n",
    "    from pyvirtualdisplay import Display\n",
    "    # opens a virtual monitor, for outputting videos\n",
    "    monitor = Display(visible=0, size=(210, 160))\n",
    "    monitor.start()\n",
    "\n",
    "\n",
    "config.model_class = CNN\n",
    "env = Wrapper(gym.make('BreakoutDeterministic-v4'))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "config.model_class = NN\n",
    "env = gym.make('CartPole-v0')\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "if record:\n",
    "    env =  gym.wrappers.Monitor(env, \"tmp\", force=True)\n",
    "config.environment = env\n",
    "#config.environment = gym.make('CartPole-v0')\n",
    "config.hyperparameters = hyperparameters[\"DQN_Agents\"]\n",
    "agent = DDQN(config)\n",
    "\n",
    "if load:\n",
    "    checkpoint = torch.load(\"./checkpoint1\")\n",
    "    agent.q_network_local.load_state_dict(checkpoint['model_dict'])\n",
    "    agent.q_network_target.load_state_dict(checkpoint['model_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\u001b[91m\u001b[1mDDQN did not achieve required score \n",
      "\u001b[0m\u001b[0m\n",
      " Episode 356, Score:  0.00, Max score seen:  9.00, Rolling score:  0.68, Max rolling score seen:  0.92\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\""
     ]
    }
   ],
   "source": [
    "save = True\n",
    "agent.set_random_seeds(90)\n",
    "# donnot reset the epsiode number (that resets eps-greddy) increase the n_episodes instead\n",
    "for i in range(50):\n",
    "    game_scores, rolling_scores, time_taken = agent.run_n_episodes(i*1000)\n",
    "    if save:\n",
    "        torch.save({'model_dict': agent.q_network_local.state_dict()}, \"checkpoint%d\"%i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f62c02d2748>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reset()\n",
    "for i in range(12):\n",
    "    state, reward, done, info  = env.step(1)\n",
    "    if done == True:\n",
    "        print(i)\n",
    "        break\n",
    "plt.imshow(state.transpose(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 84, 84)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate([x, x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S/L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"./checkpoint1\")\n",
    "agent.q_network_local.load_state_dict(checkpoint['model_dict'])\n",
    "agent.q_network_target.load_state_dict(checkpoint['model_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'model_dict': agent.q_network_local.state_dict()}, \"checkpoint4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "refer to https://openai.com/blog/openai-baselines-dqn/, for best practices and the 2013 paper https://arxiv.org/pdf/1312.5602.pdf).\n",
    "\n",
    "The MNIST and CIFAR of RL\n",
    "\n",
    "4 dim parameterized CartPole can be solved (keeping balance for more than 200 frames on average) within 500 episodes.\n",
    "LR 1e-2, tau 1e-2.\n",
    "\n",
    "Pong-ram-v0 and Pong-v0 image\n",
    "\n",
    "As for image input, BreakoutDeterministic-v4. 10M transitions is a common setting, and it is okay to have no improvements during the first 1M transitions (perhaps due to high eps). LR 1e-5, tau 1e-4, average of 11 is obtained using a single frame(incomplete state). DeepMind takes last 4 frames as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes\n",
    "Implemented real eps-greedy\n",
    "now supports setting the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
